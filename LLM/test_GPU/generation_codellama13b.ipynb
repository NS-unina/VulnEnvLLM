{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Uncomment if you haven't these packages\n",
    "%pip install --upgrade accelerate peft bitsandbytes trl huggingface_hub\n",
    "%pip install \"transformers==4.38.2\" # Bug occured in v4.39.1 - AttributeError: 'torch.dtype' object has no attribute 'itemsize'\n",
    "%pip install flash-attn --no-build-isolation #Nvidia download guide - https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path,chdir\n",
    "import sys\n",
    "chdir(path.dirname(path.realpath(sys.argv[0]))) # change working directory to script location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scripts.improve_result import improve_result\n",
    "from scripts.jsonl_parser import read_jsonl, write_jsonl\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cc5ddcdc6c41048d55b1c3a3fe8dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_name = \"Tony177/codellama-13b-dockerfile-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d8c95ef37f4aee8ce99d971d05cb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1 # Setting config.pretraining_tp to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n",
    "model.enable_input_require_grads() # Warning about gradients during generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_words_dockerfile = [\"```dockerfile\", \"```Dockerfile\"]\n",
    "forced_words_from = [\"FROM\", \"from\"]\n",
    "\n",
    "forced_words_ids = [\n",
    "    tokenizer(forced_words_dockerfile, add_special_tokens=False).input_ids,\n",
    "    tokenizer(forced_words_from, add_special_tokens=False).input_ids,\n",
    "]\n",
    "\n",
    "bad_words = [\"apk\", \"\\begin(code)\", \"\\\\end(code)\",\"EOF\", \"exit\", \"ONBUILD\"]\n",
    "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=forced_words_ids, max_new_tokens=512, num_beams=5, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_INST(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_FORCED(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=forced_words_ids, max_new_tokens=512, num_beams=5, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Wordpress 5.7 [/INST] ```dockerfile\n",
      "\n",
      "# Use the official PHP 7.4 image as the base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "\n",
      "# Set the maintainer information\n",
      "\n",
      "# Update the package lists for upgrades and new package installations\n",
      "RUN apt-get update\n",
      "\n",
      "# Install necessary packages for Wordpress\n",
      "RUN apt-get install -y --no-install-recommends \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      "        libzip-dev \\\n",
      "        zip \\\n",
      "        unzip \\\n",
      "        libpng-dev \\\n",
      "        libjpeg-dev \\\n",
      "        libpng-dev \\\n",
      " FROM\n",
      "EXPOSE 80/tcp 443/tcp\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start from the official Python 3.7 image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "\n",
      "# Set the working directory to /usr/src/app\n",
      "WORKDIR /usr/src/app\n",
      "\n",
      "# Copy the current directory contents into the container at /usr/src/app\n",
      "# COPY . /usr/src/app\n",
      "\n",
      "# Install any needed packages specified in requirements.txt\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Make port 80 available to the world outside this container\n",
      "EXPOSE 80\n",
      "\n",
      "# Define environment variable\n",
      "ENV NAME World\n",
      "\n",
      "# Run app.py when the container launches\n",
      "CMD [\"python\", \"app.py\"]\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Start from the official Ruby 3.2.1 image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "\n",
      "# Set the working directory to /app\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the Gemfile and Gemfile.lock to the working directory\n",
      "# COPY Gemfile Gemfile.lock ./\n",
      "\n",
      "# Install the dependencies\n",
      "RUN bundle install\n",
      "\n",
      "# Copy the rest of the application code to the working directory\n",
      "# COPY . .\n",
      "\n",
      "# Expose port 3000 for the application\n",
      "EXPOSE 3000\n",
      "\n",
      "# Set the default command to run when the container starts\n",
      "CMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [3:07:39<00:00, 140.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_list, output_list = read_jsonl(\"../dataset.jsonl\")\n",
    "codellama_output_list = []\n",
    "for e in tqdm(input_list):\n",
    "    codellama_output_list.append(generate_text(tokenizer, model, e))\n",
    "write_jsonl(input_list, output_list, codellama_output_list, \"../dataset_llm.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
