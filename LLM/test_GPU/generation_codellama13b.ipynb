{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Uncomment if you haven't these packages\n",
    "%pip install --upgrade accelerate peft bitsandbytes trl huggingface_hub\n",
    "%pip install \"transformers==4.38.2\" # Bug occured in v4.39.1 - AttributeError: 'torch.dtype' object has no attribute 'itemsize'\n",
    "%pip install flash-attn --no-build-isolation #Nvidia download guide - https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path,chdir\n",
    "import sys\n",
    "chdir(path.dirname(path.realpath(sys.argv[0]))) # change working directory to script location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scripts.improve_result import improve_result, generate_constraints\n",
    "from scripts.jsonl_parser import read_jsonl, write_jsonl\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0651136149d04831bae0521e8e36e9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_name = \"Tony177/codellama-13b-dockerfile-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0510f7c85c4f28be8181b43034891f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1 # Setting config.pretraining_tp to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n",
    "model.enable_input_require_grads() # Warning about gradients during generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_forced_words_ids(prompt: str) -> list:\n",
    "    image_name = generate_constraints(prompt)\n",
    "    forced_words_ids = []\n",
    "\n",
    "    if image_name != \"\":\n",
    "        forced_words_ids.append(tokenizer(f\"FROM {image_name}\\n\", add_special_tokens=True).input_ids)\n",
    "    else:\n",
    "        forced_words_ids.append(tokenizer(\"FROM\", add_special_tokens=True).input_ids)\n",
    "    forced_words_ids.append(tokenizer([\"```dockerfile\\n\", \"```Dockerfile\\n\"], add_special_tokens=True).input_ids)\n",
    "    forced_words_ids.append(tokenizer([\"ARG DEBIAN_FRONTEND noninteractive\\n\",\"ARG debian_frontend noninteractive\\n\"], add_special_tokens=True).input_ids)\n",
    "\n",
    "    return forced_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = [\"apk\", \"\\begin(code)\", \"\\\\end(code)\", \"EOF\", \"exit\", \"ONBUILD\", \"alpine\", \"# FROM\", \"#FROM\"]\n",
    "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=return_forced_words_ids(prompt), max_new_tokens=512, no_repeat_ngram_size=7, num_beams=5, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_INST(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_FORCED(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=return_forced_words_ids(prompt), max_new_tokens=512, num_beams=5, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Wordpress 5.7 [/INST] Here' ```dockerfile\n",
      " ARG DEBIAN_FRONTEND noninteractive\n",
      "\n",
      "FROM FROM wordpress:5.7\n",
      "\n",
      "# Install required packages\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "        curl \\\n",
      "        git \\\n",
      "    && apt-get clean \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install Composer\n",
      "RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer\n",
      "\n",
      "# Install Node.js\n",
      "RUN curl -sL https://deb.nodesource.com/setup_14.x | bash -\n",
      "RUN apt-get install -y nodejs\n",
      "\n",
      "# Install Yarn\n",
      "RUN npm install -g yarn\n",
      "\n",
      "# Install PHP extensions\n",
      "RUN docker-php-ext-install -j \"$(nproc)\" pdo pdo_mysql zip\n",
      "\n",
      "# Install Composer dependencies\n",
      "# COPY composer.json composer.lock /var/www/html/\n",
      "WORKDIR /var/www/html\n",
      "RUN composer install --no-autoloader --no-scripts\n",
      "\n",
      "# Install NPM dependencies\n",
      "# COPY package.json package-lock.json ./\n",
      "RUN yarn install --frozen-lockfile\n",
      "\n",
      "# Install Bower dependencies\n",
      "# COPY bower.json bower.json\n",
      "RUN bower install --allow-root --config.interactive=false\n",
      "\n",
      "# Install Grunt\n",
      "# COPY Gruntfile.js Gruntfile.js\n",
      "RUN npm install grunt-cli -g\n",
      "RUN grunt --version\n",
      "\n",
      "# Install Gulp\n",
      "# COPY gulpfile.js gulpfile.js\n",
      "# COPY gulp-config.json gulp-config.json\n",
      "RUN npm install gulp -g\n",
      "RUN gulp --version\n",
      "\n",
      "# Install Webpack\n",
      "# COPY webpack.config.js webpack.config.js\n",
      "# COPY webpack.mix.js webpack.mix.js\n",
      "# COPY mix-config.json mix-config.json\n",
      "# COPY yarn.lock yarn.lock\n",
      "RUN npm install webpack -g\n",
      "RUN webpack --version\n",
      "\n",
      "# Install Laravel Mix\n",
      "# COPY laravel-mix.json laravel-mix.json\n",
      "\n",
      "EXPOSE 80 443\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG DEBIAN_FRONTEND noninteractive\n",
      " FROM FROM python:3.7\n",
      "\n",
      " # Set the working directory to /app\n",
      " WORKDIR /app\n",
      "\n",
      " # Copy the current directory contents into the container at /app\n",
      " COPY . /app\n",
      "\n",
      " # Install any needed packages specified in requirements.txt\n",
      " RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      " # Make port 5000 available to the world outside this container\n",
      " EXPOSE 5000\n",
      "\n",
      " # Define environment variable\n",
      " ENV NAME World\n",
      "\n",
      " # Run app.py when the container launches\n",
      " CMD [\"python\", \"app.py\"]\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG DEBIAN_FRONTEND noninteractive\n",
      " FROM ruby FROM ruby:3.2.1\n",
      "\n",
      " # Install necessary packages\n",
      " RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "     build-essential \\\n",
      "     libpq-dev \\\n",
      "     libxml2-dev \\\n",
      "     libxslt-dev \\\n",
      "     libcurl4-openssl-dev \\\n",
      "     libffi-dev \\\n",
      "     nodejs \\\n",
      "     yarn \\\n",
      "     && apt-get clean \\\n",
      "     && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      " # Install bundler\n",
      " RUN gem install bundler\n",
      "\n",
      " # Set working directory\n",
      " WORKDIR /app\n",
      "\n",
      " # Copy Gemfile and Gemfile.lock\n",
      " COPY Gemfile Gemfile.lock ./\n",
      "\n",
      " # Install gems\n",
      " RUN bundle install\n",
      "\n",
      " # Copy the rest of the application code\n",
      " COPY . .\n",
      "\n",
      " # Expose port\n",
      " EXPOSE 3000\n",
      "\n",
      " # Start the application\n",
      " CMD [\"bundle\", \"exec\", \"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARG DEBIAN_FRONTEND noninteractive\n",
      "\n",
      "FROM FROM ubuntu:20.04\n",
      "\n",
      "# Install required packages\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    curl \\\n",
      "    git \\\n",
      "    unzip \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install Composer\n",
      "RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer\n",
      "\n",
      "# Install Node.js\n",
      "RUN curl -sL https://deb.nodesource.com/setup_14.x | bash -\n",
      "RUN apt-get install -y nodejs\n",
      "\n",
      "# Install Yarn\n",
      "RUN npm install -g yarn\n",
      "\n",
      "# Install uvdesk\n",
      "RUN git clone https://github.com/uvdesk/core.git /uvdesk \\\n",
      "    && cd /uvdesk && git checkout v1.1.3 \\\n",
      "    && composer install --no-dev --optimize-autoloader \\\n",
      "    && yarn install \\\n",
      "    && yarn build\n",
      "\n",
      "# Expose uvdesk port\n",
      "EXPOSE 8080\n",
      "\n",
      "# Set working directory\n",
      "WORKDIR /uvdesk\n",
      "\n",
      "# Start uvdesk\n",
      "CMD [\"php\", \"bin/console\", \"server:start\", \"0.0.0.0:8080\"]\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of uvdesk v1.1.3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [3:51:53<00:00, 173.92s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_list, output_list = read_jsonl(\"../dataset.jsonl\")\n",
    "codellama_output_list = []\n",
    "for e in tqdm(input_list):\n",
    "    codellama_output_list.append(generate_text(tokenizer, model, e))\n",
    "write_jsonl(input_list, output_list, codellama_output_list, \"../dataset_llm.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
