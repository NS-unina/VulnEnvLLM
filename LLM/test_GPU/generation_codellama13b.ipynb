{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Uncomment if you haven't these packages\n",
    "%pip install --upgrade accelerate peft bitsandbytes trl huggingface_hub\n",
    "%pip install \"transformers==4.38.2\" # Bug occured in v4.39.1 - AttributeError: 'torch.dtype' object has no attribute 'itemsize'\n",
    "%pip install flash-attn --no-build-isolation #Nvidia download guide - https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path,chdir\n",
    "import sys\n",
    "chdir(path.dirname(path.realpath(sys.argv[0]))) # change working directory to script location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scripts.improve_result import improve_result, generate_constraints\n",
    "from scripts.jsonl_parser import read_jsonl, write_jsonl\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_name = \"Tony177/codellama-13b-dockerfile-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = True\n",
    "model.config.pretraining_tp = 1 # Setting config.pretraining_tp to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n",
    "model.enable_input_require_grads() # Warning about gradients during generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_forced_words_ids(prompt: str) -> list:\n",
    "    image_name, ports =  generate_constraints(prompt)\n",
    "    forced_words_ids = []\n",
    "\n",
    "    if image_name != \"\":\n",
    "        forced_words_ids.append(tokenizer([f\"FROM {image_name}\", f\"from {image_name}\"],add_special_tokens=False).input_ids)\n",
    "    else:\n",
    "        forced_words_ids.append(tokenizer([\"FROM\", \"from\"],add_special_tokens=False).input_ids)\n",
    "\n",
    "    if ports != \"\":\n",
    "        forced_words_ids.append(tokenizer([f\"EXPOSE {ports}\", f\"expose {ports}\"],add_special_tokens=False).input_ids)\n",
    "    forced_words_ids.append(tokenizer([\"```dockerfile\", \"```Dockerfile\"],add_special_tokens=False).input_ids)\n",
    "    \n",
    "    return forced_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = [\"apk\", \"\\begin(code)\", \"\\\\end(code)\",\"EOF\", \"exit\", \"ONBUILD\"]\n",
    "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search sampling if num_beams>1 \n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=return_forced_words_ids(prompt), max_new_tokens=512, num_beams=5, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_INST(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_FORCED(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=return_forced_words_ids(prompt), max_new_tokens=512, num_beams=5, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_list, output_list = read_jsonl(\"../dataset.jsonl\")\n",
    "codellama_output_list = []\n",
    "for e in tqdm(input_list):\n",
    "    codellama_output_list.append(generate_text(tokenizer, model, e))\n",
    "write_jsonl(input_list, output_list, codellama_output_list, \"../dataset_llm.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
