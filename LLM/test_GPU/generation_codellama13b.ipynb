{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Uncomment if you haven't these packages\n",
    "%pip install --upgrade accelerate peft bitsandbytes trl huggingface_hub\n",
    "%pip install \"transformers==4.38.2\" # Bug occured in v4.39.1 - AttributeError: 'torch.dtype' object has no attribute 'itemsize'\n",
    "%pip install flash-attn --no-build-isolation #Nvidia download guide - https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path,chdir\n",
    "import sys\n",
    "chdir(path.dirname(path.realpath(sys.argv[0]))) # change working directory to script location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from scripts.improve_result import improve_result\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a6341f27a9437cae61eacd9eac155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model_name = \"Tony177/codellama-13b-dockerfile-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624343e9d7644122b082bc4dbfac44f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 # Setting config.pretraining_tp to a value different than 1 will activate the more accurate but slower computation of the linear layers, which should better match the original logits.\n",
    "model.enable_input_require_grads() # Warning about gradients during generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from Hugginface and set padding_side to “right” to fix the issue with fp16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token to tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt: str) -> str:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_INST(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_words_dockerfile = [\"```dockerfile\", \"```Dockerfile\"]\n",
    "forced_words_from = [\"FROM\", \"from\"]\n",
    "\n",
    "forced_words_ids = [\n",
    "    tokenizer(forced_words_dockerfile, add_special_tokens=False).input_ids,\n",
    "    tokenizer(forced_words_from, add_special_tokens=False).input_ids,\n",
    "]\n",
    "\n",
    "bad_words = [\"apk\",\"\\begin(code)\", \"\\\\end(code)\",\"EOF\",\"exit\"]\n",
    "bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_FORCED(tokenizer, model, prompt: str) -> str:\n",
    "    prompt = \"<s>[INST] \" + prompt + \" [/INST]\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Added last part to avoid crash to KeyError: 'shape'\n",
    "    # beam-search multinomial sampling if num_beams>1 and do_sample=True\n",
    "    gen_tokens = model.generate(input_ids, bad_words_ids=bad_words_ids , force_words_ids=forced_words_ids, max_new_tokens=512, num_beams=5, no_repeat_ngram_size=2, early_stopping=False,pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0] # One element list, just the response\n",
    "    return improve_result(prompt, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a dockerfile of Wordpress 5.7.2\n",
      "\n",
      "```\n",
      "docker build -t my-wordpress-image:latest https://raw.githubusercontent.com/your-github-username/dockerfiles/master/wordpress/Dockerfile --build-arg WORDPRESS_VERSION=5-7-2 --no-cache --force-rm=true --squash --compress --disable-content-trust --security-opt seccomp=unconfined --label maintainer=\"Your Name <your.email@domain.tld>\" --network=host --platform=linux/amd64 --pull --quiet --shm-size=1g --ulimit=nproc=65535 --userns=keep-id --volume /sys/fs/cgroup:ro --with-registry-mirror=https://dockerhub.azk8s.cn/ --add-host=kubernetes.default.svc:10.96.123.456 --cap-add=SYS_ADMIN --device=/dev/fuse --gpus all:NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 --health-cmd=\"curl --fail http://localhost:80/wp-admin/ || exit 1\" --hostname=wordpress --init=false --isolation=default --log-driver=journald --mac-address=92:d0:c6:0a:29:33 --memory-reservation=2048m --oom-score-adj=-500 --privileged --read-only --restart=unless-stopped --runtime=runc --sysctl=net.core.somaxconn=32768 --tmpfs=/run:rw,noexec,nosuid --uts=hostname:wordpress -v /var/www/html:/usr/local/apache2/htdocs:z --workdir=/home/myuser/workspace --entrypoint=/bin/sh -c \"chown -R myuser:www-data . && exec apache2 -D FOREGROUND\"\n",
      "\n",
      "EXPOSE 80/tcp 443/tcp\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Wordpress 5.7 [/INST] Here's a Dockerfile that sets up a WordPress container with PHP 7.4, Apache, and MySQL:\n",
      "\n",
      "```Dockerfile\n",
      "# Start from the official Ubuntu 20.04 (Focal Fossa) base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "ENV DEBIAN_FRONTEND=noninteractive\n",
      "RUN apt-get update && \\\n",
      "    apt install -y --no-install-recommends apache2 libapache2-mod-php mysql-server php-mysql php libzip-dev zip unzip libxml++2.6-utils libxslt1.1 libcurl4 libgd3 libmagickwand-6.q16 libpq5 libsqlite3-0 libwebp6 libyajl2 liblz4-1 libtiff-tools libxpm4 libfreetype6 fontconfig libharfbuzz0b libjbig2dec libjpeg-turbo8 libmcrypt4 mcrypt libmhash2 mhash libonig4 pkg-config zip zlib1g libicu[0-9]{0,3} libidn11 curl ca-certificates wget\n",
      "ENTRYPOINT [ \"dockerize\", \"-template\", \"/etc/wordpress/config.php.tmpl:/var/www/html/wp-content/plugins/your-plugin-name/includes/class-wc-yourpluginname-gateway-paypal-express-checkout-with-braintree-and-stripe-payment-method-for-woocommerce\" ]\n",
      "EXPOSE 80/tcp 443/tcp 3306\n",
      "WORKDIR /app\n",
      "CMD [\"php-fpm\", \"--allow-to-run-as-root\"]\n",
      "VOLUME [\"/app/php/conf.d/99-overrides.ini\",\"/data/uploads.tar.bz2\",\".htpasswd\", \".htdigest\"] \n",
      "USER root\n",
      "ONBUILD COPY --chown=www-data:0 . /data && chmod -R 0755 . && tar -xvjf uploads\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_INST(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Wordpress 5.7 [/INST] ```dockerfile\n",
      "\n",
      "# Start from a base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "ENV DEBIAN_FRONTEND noninteractive\n",
      "RUN apt-get update \\\n",
      "    && docker-php-ext-install -j$(nproc) pdo_mysql mbstring zip gd xmlrpc xml gmp bcmath intl opcache zip pcntl bz2 curl ftp gettext mcrypt mysqli json soap sockets tokenizer xmlreader xmlwriter xsl xdebug zip-opcache\n",
      "WORKDIR /var/www/html\n",
      "EXPOSE 80/tcp 443/tcp\n",
      "CMD [\"apache2-foreground\", \"-D\", \"FOREGROUND\"]\n",
      "ADD https://wordpress.org/latest.tar.gz /tmp/wp.tgz\n",
      "ARG WORDPRESS_DB_NAME=wordpress\n",
      "ENCRYPTED_PASSWORD=$(openssl rand -base64 32 | tr -d '\\n' | openssl enc -e -aes-256-cbc -a -pass pass:your_encryption_key -nosalt -K 1234 -iv 0102030405060708 -in /dev/stdin -out - | sed 's/^.//;s/.$//' && echo)\n",
      "echo \"<?php\\n\\$table_prefix = 'wp_';\\ndefine( 'WP_ALLOW_REPAIR', true );\\ndelete_site_option('update_core');\" > wp-config.php\n",
      "sed -i \"s|database_name_here|${WORLDPRESS_MYSQL_DATABASE}|g; s|yourusername_or_empty|'root'|g\" /root/.my.cnf\n",
      "mysql -u root --default-character-set=utf8mb4 --execute=\"CREATE DATABASE IF NOT EXISTS \\`$WORDLRESSDBNAME\\`; GRANT ALL PRIVILEGES ON $WORLDESSDBNAM.* TO 'root'; FLUSH privileges;\"\n",
      "tar -zxvf /tm/wptgz --strip-components=1\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_FORCED(tokenizer, model, \"Generate a dockerfile of Wordpress 5.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a dockerfile of Python 3.7.4 with the following packages installed:\n",
      "\n",
      "- numpy\n",
      "  - scipy\n",
      "    - pandas\n",
      "      - matplotlib\n",
      "        - seaborn\n",
      "          - jupyter\n",
      "            - ipywidgets\n",
      "              - bokeh\n",
      "                - tqdm\n",
      "                  \n",
      "### Dockerfile\n",
      "```\n",
      "# Use the official Python runtime as the base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "ENV PYTHONUNBUFFERED 1\n",
      "RUN apk add --no-cache --virtual .build-deps gcc musl-dev linux-headers \\\n",
      "\t&& pip install --upgrade pip setuptools wheel && rm -rf /var/cache/apk/*\n",
      "WORKDIR /usr/src/app\n",
      "CMD [\"python\", \"./your-script.py\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Python 3.7 [/INST] A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Docker uses this file to build an automated build that executes several command-line instructions in succession.\n",
      "\n",
      "In this problem, you are given a task to create a Docker image for a Python application. The image should be based on Ubuntu 18.04, and it should install the necessary dependencies, such as `build-essential`, `libpq-dev`, and `python3-pip`. It should also set the working directory to `/usr/src/app` and copy the `requirements.txt` file from the local machine to the container. After installing the dependencies and copying the requirements file, the user should run `pip3 install --no-cache-dir -r requirements.tar.gz` to install Python packages specified in the file. Finally, it copies the rest of the application code into the image and sets the ENTRYPOINT to run a bash script named `entrypoint.sh`. The script is responsible for setting up the environment and running the main Python script, `main.py`.\n",
      "```sql\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "RUN apt-get update \\\n",
      "    && DEBIAN_FRONTEND=noninteractive apt install -yq --install-recommends -o Dpkg::Options::=\"--force-confdef\" --allow-downgrades --fix-missing --auto-remove --purge --reinstall --download-only -t bionic-updates --quiet --assume-yes --yes-to-all-prerequisites-prior-installation-of-broken-or-corrupt-files --ignore-cdrom --dry-run --simulate --pretend --just-print --print-uris --show-progress --progress-fd=2 --config-file=/dev/null --admindir=/var/lib/apt/lists/ --list-cleanup --autoremove-suggests --remove-unused-dependencies --log-terminal=unsafe-io:1 --disable-ipv6 --enable-multi-arch --arch=amd64 --trust-source-on-trusted-network\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_INST(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Python 3.7 [/INST] ```dockerfile\n",
      "\n",
      "# Start from an official Python runtime as a parent image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "ENV PYTHONUNBUFFERED 1 \\\n",
      "    PIP_NO_CACHE_DIR=false\n",
      "WORKDIR /usr/src/app\n",
      "RUN pip install --no-cache-dir -U pip setuptools && pip --disable-pip-version-check install -r requirements.text\n",
      "ONBUILD COPY . .\n",
      "CMD [ \"python\", \"./your_main_script.py\" ]\n",
      "```\n",
      "This Dockerfile creates a multi-stage build with two stages. The first stage, `base`, is based on the official `python` image with Alpine Linux as the base image. It sets up the environment, installs the required dependencies, and prepares the working directory for the application.\n",
      "The second stage inherits from the first one and is used to build the actual application code. In this example, it copies the current directory's contents into the container and runs the main script. \n",
      "You can customize the `Dockerfile` to fit your specific use case, such as using a different Python version, installing additional system packages, or changing the entrypoint command. Make sure to test the resulting Docker image to ensure it meets your requirements and can run your application smoothly in the target environment (e.g., AWS Lambda, Google Cloud Functions, etc.). \n"
     ]
    }
   ],
   "source": [
    "print(generate_text_FORCED(tokenizer, model, \"Generate a dockerfile of Python 3.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a dockerfile of Ruby 3.2.1 on Alpine Linux.\n",
      "\n",
      "```Dockerfile\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "RUN apk add --no-cache --virtual .build-deps gcc libc-dev linux-headers \\\n",
      "    && docker-php-ext-install -j \"$(nproc)\" pdo_pgsql pgsql opcache bcmath pcntl zip intl mbstring gd gettext iconv xml xmlrpc xmlreader xmlwriter xsl xdebug zip zip-pdo zlib\n",
      "ENV COMPOSER_ALLOW_SUPERUSER 1\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Ruby 3.2.1 [/INST] Here's a Dockerfile that sets up a Ruby environment based on the provided snippet:\n",
      "\n",
      "```Dockerfile\n",
      "# Start from a lightweight base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "RUN apk add --no-cache --virtual .build-deps gcc libc-dev musl-libc libstdc++ \\\n",
      "    && docker-php-ext-install -j \"$(nproc)\" pdo_mysql mysqli && rm -rf /var/cache/apk/* /root/.pear /usr/local/lib/ruby/gems/*/cache/*.gem\n",
      "ENV RUBYGEMS_MIRROR https://rubygems.org/\n",
      "WORKDIR /app\n",
      "CMD [\"gem\", \"install\", \"-N\", \"/app/Gemfile\"] && bundle install --jobs 20 --retry 5 --without development test --path vendor/bundle --binstubs bin --shebang /bin/sh --deployment --verbose\n",
      "ONBUILD COPY --from=base /etc/ssl/certs/ca-certificates.crt $CA_CERTIFICATES_PATH\n",
      "EXPOSE 80 443\n",
      "ENTRYPOINT [\"ruby\"]\n",
      "STOPSIGNAL SIGTERM\n",
      "HEALTHCHECK --interval=5s --timeout=3s CMD [ -e tmp/pids/server.pid ] || exit 1\n",
      "VOLUME [\"tmp/sockets\",\"public/uploads\",\"config/settings.local.yml\",\"db/seeds.rb\",\"spec/fixtures/files\",\"log\",\"tmp\",\"storage\",\"node_modules\",\"bower_components\",\"vendor/assets/javascripts\",\".ruby-gemset\",\"coverage\",\".bundle\",\"test/dummy/public\",\"package-lock.json\",\"yarn.lock\",\"npm-debug.log\" ]\n",
      "ARG BUNDLE_APP_CONFIG=/app/.bundle/config\n",
      "USER ruby:ruby\n",
      "SHELL [\"/bin\",\"sh\",\"-lc\",\"bundle config --local without 'development:test' && exec $0 \\\"\\$@\\\"\",\"--\",\"ruby\",\"$0\",\"${1+\"$@\"}\",\"2>&1\",\"|\",\"te\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_INST(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Generate a dockerfile of Ruby 3.2.1 [/INST] ```dockerfile\n",
      "\n",
      "# Start from a lightweight base image\n",
      "FROM ubuntu:20.04\n",
      "ENV debian_frontend=noninteractive\n",
      "      org.opencontainers.image.source=\"https://github.org/your-username/docker-ruby-3-2-1\"\n",
      "SHELL [\"/bin/sh\", \"-c\"]\n",
      "ENV RUBYGEMS_MIRROR=http://rubygems.mirrors.ustc.edu.cn/\n",
      "RUN set -eux; \\ # Exit on error, print commands, and be verbose\n",
      "    addgroup -g 1001 ruby; adduser -S -D -G ruby -h /home/ruby ruby\n",
      "WORKDIR /app\n",
      "ONBUILD COPY --chown=ruby:ruby Gemfile* package.json .npmrc .yarnrc.yml .eslintrc.* .prettierrc* .browserslistrc babel.config.js tsconfig.* jest.* webpack.* karma.* rollup.* postcss.* eslint.* stylelint* jest-puppeteer.* cypress.* pre-commit* lint-staged* commit-msg* npm-shrinkwrap* yarn.lock pnpm-lock.yaml .dockerignore .editorconfig .gitattributes .github .vscode .husky .nyc_output .nvm .node_repl_history .releaserc .renovaterc .rubocop .rspec .styleci .travis .watchmanconfig appveyor* circleci* codeship* codecov* coveralls* david* deepsource* dependabot* github-actions* greenkeeper* heroku* jenkins* semantic-release* sonar* snyk* teamcity* travis* verdaccio* vercel* vsts* webpack-bundle-analyzer* 2> /dev/null || true\n",
      "USER ruby:root\n",
      "```\n",
      "This Dockerfile creates a multi-stage build with two stages. The first stage, `base`, is based on the `alpine` image and sets up a user named `ruby`. The second stage inherits from the first one, copies the necessary files, sets the working directory to `/app`,\n"
     ]
    }
   ],
   "source": [
    "print(generate_text_FORCED(tokenizer, model, \"Generate a dockerfile of Ruby 3.2.1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0.0 (Python 3.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
